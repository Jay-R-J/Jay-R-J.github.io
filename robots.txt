# Robots.txt 文件 - 搜索引擎爬虫规则
User-agent: *

# 允许访问所有公开内容
Allow: /

# 禁止访问的非公开目录和文件
Disallow: /_drafts/
Disallow: /node_modules/
Disallow: /themes/
Disallow: /source/
Disallow: /package.json
Disallow: /_config.yml
Disallow: /*.md$  

# 网站地图 - 帮助搜索引擎发现所有页面
Sitemap: https://jay-r-j.github.io/sitemap.xml
Sitemap: https://jay-r-j.github.io/atom.xml

# 爬虫抓取频率 - 合理设置抓取间隔
Crawl-delay: 5

# 允许主要搜索引擎爬虫
User-agent: Baiduspider
Allow: /
Crawl-delay: 3

User-agent: Googlebot
Allow: /
Crawl-delay: 3

User-agent: Bingbot
Allow: /
Crawl-delay: 3

User-agent: YandexBot
Allow: /
Crawl-delay: 3

